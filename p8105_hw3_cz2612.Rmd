---
title: "Homework 3"
author: "Cong Zhang"
date: 2020-10-09
output: github_document
---

This is my solution to Homework 3.

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
knitr::opts_chunk$set(
	fig.width = 6, 
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


## Problem 1

```{r}
data("instacart")
```

This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns. 

Observations are the level of items in orders by user. There are user / order variables -- user ID, order ID, order day, and order hour. There are also item variables -- name, aisle, department, and some numeric codes. 

How many aisles, and which are most items from?

```{r}
instacart %>% 
	count(aisle) %>% 
	arrange(desc(n))
```


Let's make a plot

```{r}
instacart %>% 
	count(aisle) %>% 
	filter(n > 10000) %>% 
	mutate(
		aisle = factor(aisle),
		aisle = fct_reorder(aisle, n)
	) %>% 
	ggplot(aes(x = aisle, y = n)) + 
	geom_point() + 
	theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```


Let's make a table!!

```{r}
instacart %>% 
	filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
	group_by(aisle) %>% 
	count(product_name) %>% 
	mutate(rank = min_rank(desc(n))) %>% 
	filter(rank < 4) %>% 
	arrange(aisle, rank) %>% 
	knitr::kable()
```


Apples vs ice cream..

```{r}
instacart %>% 
	filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
	group_by(product_name, order_dow) %>% 
	summarize(mean_hour = mean(order_hour_of_day)) %>% 
	pivot_wider(
		names_from = order_dow,
		values_from = mean_hour
	)
```


## Problem 2

Load, tidy, and otherwise wrangle the data.

```{r accel_df, message = FALSE}
accel_df = 
	read_csv("./data/accel_data.csv") %>% 
	janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute_raw",
    values_to = "activity_counts"
  ) %>% 
  separate(minute_raw, into = c("raw", "minute"), convert = TRUE) %>% 
	mutate(
	  week = as.integer(week),
	  day_id = as.integer(day_id),
	  day = as.factor(day),
	  day = ordered(day, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")),
    weekday = case_when(
      day %in% c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday") ~ "weekday",
      day %in% c("Saturday", "Sunday") ~ "weekend",
      TRUE ~ ""
    ),
	  weekday = as.factor(weekday)
	) %>% 
	select(week:day, weekday, minute, activity_counts)
```

The resulting dataset `accel_df` contains the following `r ncol(accel_df)` variables: `r names(accel_df)`. It has `r nrow(accel_df)` observations, and the dimension is `r nrow(accel_df)` x `r ncol(accel_df)`.


Create daily total activity variable and corresponding table.

```{r total_activity}
accel_df %>% 
  group_by(week, day) %>%
  summarize(total_activity = sum(activity_counts)) %>% 
  knitr::kable(digits = 2)
```

From the table, we could see that the activities on Tuesdays and Wednesdays are more stable than other days.


Make a single-panel plot.

```{r activity_plot}
ggp_activity_plot = 
  accel_df %>% 
  ggplot(aes(x = minute, y = activity_counts, color = day)) + 
  geom_line(alpha = .5) + 
  geom_smooth(se = FALSE) + 
  labs(
    title = "Activity counts plot",
    x = "Minute of the day",
    y = "Activity counts",
    caption = "Data from the accel_data.csv file"
  ) + 
  viridis::scale_color_viridis(
    name = "Day", 
    discrete = TRUE
  )

ggp_activity_plot
```

From this graph, we could see that the activity counts of night are normally larger than the activity counts of the daytime, and the activity counts of midnight are the lowest of all day. Generally speaking, Friday night and Sunday morning seem to have higher activity counts than other time.


## Problem 3

Load and describe the data.

```{r}
data("ny_noaa")
```

The dataset `ny_noaa` contains the following `r ncol(ny_noaa)` variables: `r names(ny_noaa)`. It has `r nrow(ny_noaa)` observations, and the dimension is `r nrow(ny_noaa)` x `r ncol(ny_noaa)`. This dataset has many missing values. prcp has `r sum(is.na(pull(ny_noaa, prcp)))` missing values.  snow has `r sum(is.na(pull(ny_noaa, snow)))` missing values.  snwd has `r sum(is.na(pull(ny_noaa, snwd)))` missing values.  tmax has `r sum(is.na(pull(ny_noaa, tmax)))` missing values.  tmin has `r sum(is.na(pull(ny_noaa, tmin)))` missing values. 


Clean the NY NOAA data.

```{r ny_noaa}
ny_noaa = 
	ny_noaa %>% 
	janitor::clean_names() %>% 
  separate(date, into = c("year", "month", "day"), convert = TRUE) %>% 
	mutate(
	  tmax = as.numeric(tmax),
	  tmin = as.numeric(tmin),
	  prcp = prcp / 10,
	  tmax = tmax / 10,
	  tmin = tmin / 10,
  )
```

```{r snow_freq_df}
snow_freq_df =
  ny_noaa %>%
  group_by(snow) %>%
  summarize(snow_freq = n()) %>% 
  arrange(desc(snow_freq)) %>% 
  head(n = 1)
```

For snowfall, the most commonly observed values are `r max(pull(snow_freq_df, snow))`, because there are `r max(pull(snow_freq_df, snow_freq))` observations have a snow value of `r max(pull(snow_freq_df, snow))`, which are the most frequent.


